---
title: "CaseStudy_Vrushali Banda"
author: "Vrushali Banda"
date: "12/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
```{r}


df<- read.csv("C:/Users/rajja/Downloads/GermanCredit.csv")
head(df)
missing(df)
```

## 1. Review the predictor variables and guess what their role in a credit decision might be. Are there any surprise in the data?
```{r}
summary(df) 
df$PRESENT_RESIDENT <- df$PRESENT_RESIDENT - 1
df$PRESENT_RESIDENT

t(t(colnames(df)))
df <- df[,c(-1,-22)]
head(df)

df$OTHER_PURPOSE <- ifelse(df$NEW_CAR+df$USED_CAR+df$FURNITURE+df$RADIO.TV+df$EDUCATION+df$RETRAINING==0, 1, 0)
df$Female <- ifelse(df$MALE_DIV+df$MALE_MAR_or_WID+df$MALE_SINGLE==0, 1, 0)

df$PRESENT_RESIDENT <- factor(df$PRESENT_RESIDENT, levels = c(0, 1, 2, 3), labels=c("<=1_year","1-2_years","2-3_year",">=3_years"))
df$EMPLOYMENT <- factor(df$EMPLOYMENT, levels = c(0,1,2,3,4), labels = c("Unempoyed", "<1year","1-3year","4-6year",">=7years"))
df$JOB <- factor(df$JOB, levels = c(0, 1, 2, 3), labels=c("Uemployed/unskilled-nonresident","Unskilled-resident","Skilled employee/official","Management/self-employed/highly-qualified-employee/officer"))
df$CHK_ACCT <- factor(df$CHK_ACCT, levels=c(0,1,2,3), labels = c("<0DM","0-200DM","200DM","No_checking_account"))
df$HISTORY <- factor(df$HISTORY, levels = c(0,1,2,3,4), labels = c("No_credits_taken","All_paid","Existing_paid","Delay","Critical_account"))
df$SAV_ACCT <- factor(df$SAV_ACCT, levels=c(0,1,2,3,4), labels = c("<
                                                              100DM","101-500DM","501-1000DM","1000DM","Unknown/no_saving_account"))
updated_df <- df
```
#"Resonse" column is the target variable, so it is a classification problem and the rest are independent variables we need to review.
1.Present_Resident has category 4 which doesn't exist in the choices,so need to substract by 1 tp get 0-3.
2.For the variables REAL_ESTATE and PROP_UNKN_NONE, two of which are compliment. so if chose own real estate, the other one will be 0. in this case, we can omit PROP_UNKN_NONE
3.There are no others purpose choice, so to complete the selection, an OTHER_PURPOSE is added to the data set.
4.For the sex choice, no female was in the data set. an Female choice is added. 

#FOREIGN is the surprising predictor varible in the data set.
#a.Not a foreigner and has a bad credit - 296
#b.Not a foreigner and has a good credit - 668
#c.Is a foreigner and has a bad credit - 4  
#d.Is a foreigner and has a good credit - 34
#From the above records it can be seen that the percentage of foreign people having a good credit rating is way higher than the local people.



## 2. Divide the data into trainning and validatin partitions, and develop classification models using following data mining techniques in R: logistic regression, classification trees, and neural networks.
```{r}
#data partition
set.seed(1)
dim(df)
training_rows <- sample(c(1:1000), 600)
train_data <- df[training_rows,]
valid_data <- df[-training_rows,]


#logistic regression model
glm <- glm(RESPONSE~., data = train_data, family="binomial")
options(scipen = 999)
summary(glm)
head(valid_data)
t(t(colnames(valid_data)))
pred <- predict(glm, valid_data[,-30], type = "response")
pred
library(caret)
confusionMatrix(as.factor(ifelse(pred>0.5, 1, 0)), as.factor(valid_data$RESPONSE))

#By using logistic regression model,
#Cost Matrix:
#              Actual
#             Bad            Good
# Predited
# Bad         0              100*29=2900   
# Good     62*500=31000       0
 
# Gain Matrix:
#              Actual
#             Bad           Good
# Predicted    
# Bad          0             0
# Good      -500*62=-31000    100*250=25000

#Logistic regression net profit is -6,000

```

```{r}
#select predictors and predict response of validation data.
head(valid_data)
train_glm <- glm(RESPONSE~CHK_ACCT+SAV_ACCT+INSTALL_RATE+DURATION+HISTORY+OTHER_INSTALL+FOREIGN, data=train_data, family="binomial")
valid_pred <- predict(train_glm, valid_data[,-30], type = "response")

confusionMatrix(as.factor(ifelse(valid_pred>0.5, 1, 0)), as.factor(valid_data$RESPONSE))
```

#classification trees
```{r}
#partition data for trees
library(rpart) 
library(rpart.plot)
head(updated_df)
set.seed(1)
training_rows <- sample(c(1:1000), 600)
train_data_tree <- updated_df[training_rows,]
valid_data_tree <- updated_df[-training_rows,]

#classification tree model
train_tree <- rpart(RESPONSE ~ ., data = train_data_tree, minbucket = 50, maxdepth = 10, model=TRUE, method = "class")
train_tree
train_tree$cptable
train_tree$cptable[which.min(train_tree$cptable[,"xerror"]),"CP"]
pfit_tree <- prune(train_tree, cp = train_tree$cptable[which.min(train_tree$cptable[,"xerror"]),"CP"])
pfit_tree
prp(train_tree) 

# predictions on validation set 
pred_valid <- predict(train_tree, valid_data[,-30])
pred_valid[,2]
confusionMatrix(as.factor(1*(pred_valid[,2]>0.5)), as.factor(valid_data$RESPONSE), positive = "1")

#By using classification tree model,
# Cost Matrix:
#              Actual
#             Bad            Good
# Predited
# Bad         0              100*46=4600   
# Good     65*500=32500       0
# Gain Matrix:
#              Actual
#              Bad           Good
# Predicted    
# Bad          0             0
# Good      -500*65=-32500    100*233=23300

#Classification tree net profit is -9,800

```

#neural networks
```{r}
library("neuralnet")

nn_df<-read.csv("C:/Users/rajja/Downloads/GermanCredit.csv")
head(nn_df)
scale <- preProcess(nn_df, method = c("range"))
df_scale <- predict(scale, nn_df)
df_scale$good_credit <- df_scale$RESPONSE == 1
df_scale$bad_credit <- df_scale$RESPONSE == 0

set.seed(1)
training_rows <- sample(c(1:1000), 600)
train_data_nn <- df_scale[training_rows,]
valid_data_nn <- df_scale[-training_rows,]
head(valid_data_nn)
colnames(train_data_nn)[8] <- "RADIO_OR_TV"
colnames(train_data_nn)[18] <- "COAPPLICANT" 
colnames(train_data_nn)
nn <- neuralnet(bad_credit+good_credit~CHK_ACCT+DURATION+HISTORY+NEW_CAR+USED_CAR+FURNITURE+RADIO_OR_TV+EDUCATION+RETRAINING+AMOUNT+SAV_ACCT+EMPLOYMENT+INSTALL_RATE+MALE_DIV+MALE_SINGLE+MALE_MAR_or_WID+COAPPLICANT+GUARANTOR+PRESENT_RESIDENT+REAL_ESTATE+PROP_UNKN_NONE+AGE+OTHER_INSTALL+RENT+OWN_RES+NUM_CREDITS+JOB+NUM_DEPENDENTS+TELEPHONE+FOREIGN, data = train_data_nn, linear.output = F, hidden = 3)
nn$net.result
plot(nn)
plot(nn, rep="best")
t(t(colnames(train_data_nn)))
predict <- compute(nn, valid_data_nn[,2:31])

predict$net.result
apply(predict$net.result,1,which.max)
predicted.class <- apply(predict$net.result,1,which.max)-1
predicted.class
confusionMatrix(as.factor(predicted.class), as.factor(valid_data_nn$RESPONSE))

# By using neural network model,
#''Cost Matrix:
 #             Actual
#            Bad            Good
# Predited
# Bad         0              100*66=6600   
#Good     66*500=33,000       0
# Gain Matrix:
#              Actual
#              Bad           Good
#Predicted    
# Bad          0             0
# Good      -500*66=-33000    100*213=21300'''

#Neural Network net profit is -11,700

#By comparing 3 models,logistic regression model has the best net profit/lowest losses.

```

4.Let's try and improve our performance. Rather than accept the default classification of all applicants' credit status, use the estimated probabilities (propensitues) from the logistic regression (where success means 1) as a basis for selecting the best credit risks first, followed by poorer-risk applicants. Create a vector containing the net profit for each record in the validation set. Use this vestor to create a decile-wise lift chart for the validation set that incorporates the net profit.
#a.How far into the validation data should you go to get maximum net profit? (often, this is specified as a percentile or rounded to deciles.) 

```{r}
netprofit_df <- data.frame(Predicted = pred, Actual = valid_data$RESPONSE)
netprofit_df <- netprofit_df[order(-netprofit_df$Predicted),]
netprofit_df$net_profit <- netprofit_df$Actual*100
netprofit_df$cum_net_profit <- cumsum(netprofit_df$net_profit)
head(netprofit_df)


#Create a vector containing the net profit for each record in the validation set
net_profit <- as.vector(netprofit_df$net_profit)
#Question a:
#compute deciles and plot decile-wise chart
library(gains)
gain <- gains(net_profit, netprofit_df$Predicted, groups=10)

heights <- gain$mean.resp/mean(netprofit_df$Actual)
midpoints <- barplot(heights, names.arg = gain$depth, ylim = c(0,150), 
                     xlab = "Percentile", ylab = "Mean Response", main = "Decile-wise lift chart")
text(midpoints, heights+0.5, labels=round(heights, 1), cex = 0.8)

#The decile chart indicates that we can use the model to select the top 40% data with the highest propensities to get maximum net profit.


```
#b.if this logistic regression model is used to score to future applicants, what "probability of success" cutoff should be used in extending credit?
```{r}
# plot lift chart
plot(c(0,gain$cume.pct.of.total*sum(netprofit_df$Actual))~c(0,gain$cume.obs), 
     xlab="# cases", ylab="Cumulative", main="", type="l")
lines(c(0,sum(netprofit_df$Actual))~c(0, dim(netprofit_df)[1]), lty=2)
# plot a ROC curve
library(pROC)
r <- roc(netprofit_df$Actual, netprofit_df$Predicted)
plot.roc(r)
auc(r)

cut_off <- netprofit_df$Predicted[round(length(netprofit_df$Predicted)*0.4)]
cut_off
# Answer of b: 0.832 cutoff value should be used in extending credit

# try cutoff value in the model
confusionMatrix(as.factor(ifelse(netprofit_df$Predicted>0.832, 1, 0)), as.factor(netprofit_df$Actual))
# try out different cutoff values to examine.
confusionMatrix(as.factor(ifelse(netprofit_df$Predicted>0.8, 1, 0)), as.factor(netprofit_df$Actual))
#net profit: 16700-10000=6700
confusionMatrix(as.factor(ifelse(netprofit_df$Predicted>0.9, 1, 0)), as.factor(netprofit_df$Actual))
#net profit : 10900-3000=7600
confusionMatrix(as.factor(ifelse(netprofit_df$Predicted>0.7, 1, 0)), as.factor(netprofit_df$Actual))
#net profit : 20600-34*500=20600-17000=3600

# the cost matrix with cutoff value of 0.892
# Cost Matrix:
#              Actual
#             Bad            Good
# Predited
# Bad         0             100*128=12800   
# Good     9*500=4500       0
# Gain Matrix:
#              Actual
#              Bad           Good
# Predicted    
# Bad          0             0
# Good      -500*6=-4,500   100*151=15,100
# Although the accuracy of the model is low, 
# but the net profit is calculated as positive 10,600 by setting the cutoff value of 0.832.
```
#In conclusion, the best model is the logistic regression model. and by computing the cost and net profit of the ideal model, we found that the model with highest accruracy can not benefit the bank. by checking with decile-wise chart, top 40% of the data with highest propensity will give us the largest net profit(with actual RESPONSE). So we can either set the cutoff value at 0.832(This can also been seen on a ROC curve plot) or use the top 40% of the validation data with top propensity to make a decision.